"""
ðŸ”— ÙØ§Ø­Øµ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ØªÙ‚Ø¯Ù… | Advanced Link Scanner
ÙŠØ¯Ø®Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø· ÙØ¹Ù„ÙŠØ§Ù‹ ÙˆÙŠØ­Ù„Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰!

Ø§Ù„ÙØ­ÙˆØµØ§Øª:
1. ØªØ­Ù„ÙŠÙ„ URL (Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†ØŒ Ø§Ù„Ù…Ø³Ø§Ø±)
2. ðŸ†• ÙØªØ­ Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©
3. ðŸ†• ÙƒØ´Ù ØµÙØ­Ø§Øª Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ÙˆØ§Ù„ØªØµÙŠØ¯
4. ðŸ†• ÙˆØµÙ ÙˆØ§Ø¶Ø­ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ
"""

import re
import httpx
from urllib.parse import urlparse
from typing import List, Dict
from bs4 import BeautifulSoup

# ==================== Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø´Ø¨ÙˆÙ‡Ø© ====================
SUSPICIOUS_TLDS = ['.xyz', '.top', '.click', '.loan', '.work', '.date', '.racing', '.download', '.gdn', '.win', '.bid', '.trade']

URL_SHORTENERS = ['bit.ly', 'tinyurl.com', 't.co', 'goo.gl', 'ow.ly', 'is.gd', 'buff.ly', 'cutt.ly', 'rb.gy', 'shorturl.at']

# Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ© Ù„Ù„Ø§Ù†ØªØ­Ø§Ù„
TARGETED_BRANDS = {
    'paypal': ['paypa1', 'paypai', 'paypaI', 'paipal', 'paypall', 'pay-pal'],
    'apple': ['app1e', 'appie', 'applÐµ', 'apple-id', 'icloud-verify'],
    'microsoft': ['micros0ft', 'microsft', 'ms-login', 'outlook-verify'],
    'google': ['g00gle', 'googie', 'google-verify', 'gmail-secure'],
    'amazon': ['amaz0n', 'amazn', 'amazon-prime'],
    'Ø§Ù„Ø±Ø§Ø¬Ø­ÙŠ': ['alrajhi-bank', 'rajhi-secure', 'alrajhi-update', 'rajhi-verify'],
    'Ø§Ù„Ø£Ù‡Ù„ÙŠ': ['alahli-bank', 'ahli-secure', 'snb-update', 'alahli-verify'],
    'stc': ['stc-pay', 'stc-reward', 'mystc-update', 'stc-verify'],
    'Ø§Ù„Ø¥Ù†Ù…Ø§Ø¡': ['alinma-bank', 'inma-secure'],
    'Ø§Ù„Ø¨Ù„Ø§Ø¯': ['albilad-bank', 'bilad-secure']
}


def extract_urls(text: str) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ù†Øµ"""
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
    urls = re.findall(url_pattern, text)
    cleaned = []
    for url in urls:
        url = url.rstrip('.,;:!?)')
        if len(url) > 10:
            cleaned.append(url)
    return list(set(cleaned))


def analyze_url_syntax(url: str) -> Dict:
    """ØªØ­Ù„ÙŠÙ„ Ø´ÙƒÙ„ Ø§Ù„Ø±Ø§Ø¨Ø· ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† ÙØªØ­Ù‡)"""
    result = {
        "url": url,
        "risk_score": 0,
        "flags": [],
        "domain": "",
        "is_shortened": False,
        "is_suspicious_tld": False,
        "impersonating": None
    }
    
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        result["domain"] = domain
        
        for tld in SUSPICIOUS_TLDS:
            if domain.endswith(tld):
                result["is_suspicious_tld"] = True
                result["risk_score"] += 25
                result["flags"].append(f"Ù†Ø·Ø§Ù‚ Ù…Ø´Ø¨ÙˆÙ‡ ({tld})")
                break
        
        for shortener in URL_SHORTENERS:
            if shortener in domain:
                result["is_shortened"] = True
                result["risk_score"] += 20
                result["flags"].append("Ø±Ø§Ø¨Ø· Ù…Ø®ØªØµØ± ÙŠØ®ÙÙŠ Ø§Ù„ÙˆØ¬Ù‡Ø©")
                break
        
        for brand, fakes in TARGETED_BRANDS.items():
            for fake in fakes:
                if fake in domain:
                    result["impersonating"] = brand
                    result["risk_score"] += 40
                    result["flags"].append(f"Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù†ØªØ­Ø§Ù„ {brand}")
                    break
        
        if not url.startswith('https://'):
            result["risk_score"] += 15
            result["flags"].append("Ø¨Ø¯ÙˆÙ† ØªØ´ÙÙŠØ± HTTPS")
        
        if re.match(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', domain):
            result["risk_score"] += 30
            result["flags"].append("ÙŠØ³ØªØ®Ø¯Ù… IP Ø¨Ø¯Ù„ Ø¯ÙˆÙ…ÙŠÙ†")
            
    except:
        pass
    
    return result


async def fetch_and_analyze_content(url: str, timeout: float = 10.0) -> Dict:
    """
    ðŸ”¥ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: ØªÙØªØ­ Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØªØ­Ù„Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰!
    """
    result = {
        "url": url,
        "accessible": False,
        "final_url": None,
        "redirected": False,
        "page_title": None,
        "page_description": None,
        "content_type": None,
        "has_login_form": False,
        "has_password_field": False,
        "has_email_field": False,
        "has_card_fields": False,
        "has_otp_field": False,
        "has_download_button": False,
        "form_action_external": False,
        "fields_detected": [],
        "arabic_description": "",
        "content_summary": "",
        "risk_score": 0,
        "flags": []
    }
    
    try:
        async with httpx.AsyncClient(
            follow_redirects=True, 
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
        ) as client:
            
            response = await client.get(url)
            
            if response.status_code == 200:
                result["accessible"] = True
                result["final_url"] = str(response.url)
                
                if str(response.url) != url:
                    result["redirected"] = True
                    result["flags"].append(f"ØªÙ… ØªÙˆØ¬ÙŠÙ‡Ùƒ Ù„Ù€: {response.url.host}")
                    result["risk_score"] += 15
                
                html = response.text
                soup = BeautifulSoup(html, 'html.parser')
                
                # Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø©
                title_tag = soup.find('title')
                if title_tag:
                    result["page_title"] = title_tag.get_text().strip()[:100]
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙˆØ±Ù…Ø§Øª ÙˆØ§Ù„Ø­Ù‚ÙˆÙ„
                all_inputs = soup.find_all('input')
                fields = []
                
                for inp in all_inputs:
                    inp_type = inp.get('type', '').lower()
                    inp_name = inp.get('name', '').lower()
                    inp_placeholder = inp.get('placeholder', '').lower()
                    inp_id = inp.get('id', '').lower()
                    
                    all_attrs = f"{inp_type} {inp_name} {inp_placeholder} {inp_id}"
                    
                    # ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ±
                    if inp_type == 'password' or 'password' in all_attrs or 'pass' in all_attrs:
                        result["has_password_field"] = True
                        if "ðŸ”‘ ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ±" not in fields:
                            fields.append("ðŸ”‘ ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ±")
                    
                    # Ø¥ÙŠÙ…ÙŠÙ„
                    if inp_type == 'email' or 'email' in all_attrs or 'mail' in all_attrs:
                        result["has_email_field"] = True
                        if "ðŸ“§ Ø¨Ø±ÙŠØ¯ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ" not in fields:
                            fields.append("ðŸ“§ Ø¨Ø±ÙŠØ¯ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ")
                    
                    # Ø¨Ø·Ø§Ù‚Ø© Ø¨Ù†ÙƒÙŠØ©
                    if any(x in all_attrs for x in ['card', 'credit', 'cvv', 'cvc', 'expir', 'Ø¨Ø·Ø§Ù‚Ø©']):
                        result["has_card_fields"] = True
                        if "ðŸ’³ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø·Ø§Ù‚Ø© Ø¨Ù†ÙƒÙŠØ©" not in fields:
                            fields.append("ðŸ’³ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø·Ø§Ù‚Ø© Ø¨Ù†ÙƒÙŠØ©")
                    
                    # OTP
                    if any(x in all_attrs for x in ['otp', 'code', 'verify', 'token', 'Ø±Ù…Ø²']):
                        result["has_otp_field"] = True
                        if "ðŸ”¢ Ø±Ù…Ø² ØªØ­Ù‚Ù‚ OTP" not in fields:
                            fields.append("ðŸ”¢ Ø±Ù…Ø² ØªØ­Ù‚Ù‚ OTP")
                    
                    # Ø¬ÙˆØ§Ù„
                    if any(x in all_attrs for x in ['phone', 'mobile', 'tel', 'Ø¬ÙˆØ§Ù„']):
                        if "ðŸ“± Ø±Ù‚Ù… Ø¬ÙˆØ§Ù„" not in fields:
                            fields.append("ðŸ“± Ø±Ù‚Ù… Ø¬ÙˆØ§Ù„")
                    
                    # Ù‡ÙˆÙŠØ©
                    if any(x in all_attrs for x in ['ssn', 'national', 'Ù‡ÙˆÙŠØ©']):
                        if "ðŸªª Ø±Ù‚Ù… Ù‡ÙˆÙŠØ©" not in fields:
                            fields.append("ðŸªª Ø±Ù‚Ù… Ù‡ÙˆÙŠØ©")
                    
                    # Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù…
                    if any(x in all_attrs for x in ['user', 'login', 'username']):
                        if "ðŸ‘¤ Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù…" not in fields:
                            fields.append("ðŸ‘¤ Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù…")
                
                result["fields_detected"] = fields
                
                # ÙƒØ´Ù Ù†ÙˆØ¹ Ø§Ù„ØµÙØ­Ø©
                html_lower = html.lower()
                
                if result["has_password_field"]:
                    result["has_login_form"] = True
                    result["content_type"] = "login"
                    result["risk_score"] += 30
                
                if result["has_card_fields"]:
                    result["content_type"] = "payment"
                    result["risk_score"] += 50
                
                if any(x in html_lower for x in ['download', 'ØªØ­Ù…ÙŠÙ„', '.exe', '.apk']):
                    result["has_download_button"] = True
                    result["content_type"] = "download"
                    result["risk_score"] += 25
                
                # ÙØ­Øµ action Ø§Ù„ÙÙˆØ±Ù…
                forms = soup.find_all('form')
                for form in forms:
                    action = form.get('action', '')
                    if action and not action.startswith('/') and not action.startswith('#'):
                        parsed_action = urlparse(action)
                        parsed_url = urlparse(url)
                        if parsed_action.netloc and parsed_action.netloc != parsed_url.netloc:
                            result["form_action_external"] = True
                            result["flags"].append("Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØ±Ø³Ù„ Ù„Ù…ÙˆÙ‚Ø¹ Ø®Ø§Ø±Ø¬ÙŠ!")
                            result["risk_score"] += 30
                
                # Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙˆØµÙ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ
                result["arabic_description"] = build_arabic_description(result)
                result["content_summary"] = build_content_summary(result)
                
            else:
                result["flags"].append(f"Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø±Ø¬Ø¹ Ø®Ø·Ø£: {response.status_code}")
                
    except httpx.TimeoutException:
        result["flags"].append("Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ø·ÙŠØ¡ Ø¬Ø¯Ø§Ù‹")
        result["risk_score"] += 10
    except Exception as e:
        result["flags"].append("ØªØ¹Ø°Ø± Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…ÙˆÙ‚Ø¹")
    
    result["risk_score"] = min(result["risk_score"], 100)
    return result


def build_arabic_description(analysis: Dict) -> str:
    """Ø¨Ù†Ø§Ø¡ ÙˆØµÙ ÙˆØ§Ø¶Ø­ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
    parts = []
    
    if analysis.get("content_type") == "login":
        parts.append("ðŸ“„ Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠÙØªØ­ ØµÙØ­Ø© ØªØ³Ø¬ÙŠÙ„ Ø¯Ø®ÙˆÙ„")
    elif analysis.get("content_type") == "payment":
        parts.append("ðŸ’³ Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠÙØªØ­ ØµÙØ­Ø© Ø¯ÙØ¹/Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†ÙƒÙŠØ©")
    elif analysis.get("content_type") == "download":
        parts.append("â¬‡ï¸ Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠÙØªØ­ ØµÙØ­Ø© ØªØ­Ù…ÙŠÙ„")
    
    fields = analysis.get("fields_detected", [])
    if fields:
        parts.append(f"\n\nðŸ” Ø§Ù„ØµÙØ­Ø© ØªØ·Ù„Ø¨ Ù…Ù†Ùƒ:\n" + "\n".join([f"  â€¢ {f}" for f in fields]))
    
    warnings = []
    if analysis.get("has_password_field"):
        warnings.append("ÙŠØ·Ù„Ø¨ ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ±Ùƒ")
    if analysis.get("has_card_fields"):
        warnings.append("ÙŠØ·Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø·Ø§Ù‚ØªÙƒ Ø§Ù„Ø¨Ù†ÙƒÙŠØ©!")
    if analysis.get("redirected"):
        warnings.append(f"ØªÙ… ØªÙˆØ¬ÙŠÙ‡Ùƒ Ù„Ù…ÙˆÙ‚Ø¹ Ø¢Ø®Ø±")
    if analysis.get("form_action_external"):
        warnings.append("Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØ±Ø³Ù„ Ù„Ù…ÙˆÙ‚Ø¹ Ø®Ø§Ø±Ø¬ÙŠ!")
    
    if warnings:
        parts.append(f"\n\nâš ï¸ ØªØ­Ø°ÙŠØ±Ø§Øª:\n" + "\n".join([f"  â€¢ {w}" for w in warnings]))
    
    if analysis.get("page_title"):
        parts.append(f"\n\nðŸ“Œ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø©: {analysis['page_title']}")
    
    if not parts:
        if analysis.get("accessible"):
            return "âœ… ØµÙØ­Ø© Ø¹Ø§Ø¯ÙŠØ© Ø¨Ø¯ÙˆÙ† Ø·Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø§Ø³Ø©"
        return "âŒ ØªØ¹Ø°Ø± Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø±Ø§Ø¨Ø·"
    
    return "".join(parts)


def build_content_summary(analysis: Dict) -> str:
    """Ù…Ù„Ø®Øµ Ù‚ØµÙŠØ±"""
    if analysis.get("has_card_fields"):
        return "ðŸš¨ ØµÙØ­Ø© ØªØ·Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø·Ø§Ù‚Ø© Ø¨Ù†ÙƒÙŠØ©!"
    if analysis.get("has_password_field") and analysis.get("has_email_field"):
        return "âš ï¸ ØµÙØ­Ø© ØªØ³Ø¬ÙŠÙ„ Ø¯Ø®ÙˆÙ„ ØªØ·Ù„Ø¨ Ø¥ÙŠÙ…ÙŠÙ„ ÙˆÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ±"
    if analysis.get("has_password_field"):
        return "âš ï¸ ØµÙØ­Ø© ØªØ·Ù„Ø¨ ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ±"
    if analysis.get("has_otp_field"):
        return "âš ï¸ ØµÙØ­Ø© ØªØ·Ù„Ø¨ Ø±Ù…Ø² ØªØ­Ù‚Ù‚ OTP"
    if analysis.get("has_download_button"):
        return "â¬‡ï¸ ØµÙØ­Ø© ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª"
    if analysis.get("redirected"):
        return f"â†ªï¸ ØªÙ… Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ù„Ù…ÙˆÙ‚Ø¹ Ø¢Ø®Ø±"
    if analysis.get("accessible"):
        return "âœ… ØµÙØ­Ø© Ø¹Ø§Ø¯ÙŠØ©"
    return "â“ ØªØ¹Ø°Ø± Ø§Ù„ÙØ­Øµ"


async def full_link_analysis(url: str) -> Dict:
    """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„: syntax + Ù…Ø­ØªÙˆÙ‰"""
    syntax = analyze_url_syntax(url)
    content = await fetch_and_analyze_content(url)
    
    total_risk = min(syntax["risk_score"] + content["risk_score"], 100)
    all_flags = syntax["flags"] + content["flags"]
    
    if total_risk >= 70:
        verdict = "ðŸš¨ Ø®Ø·ÙŠØ± Ø¬Ø¯Ø§Ù‹ - Ù„Ø§ ØªØ¯Ø®Ù„!"
        verdict_class = "danger"
    elif total_risk >= 40:
        verdict = "âš ï¸ Ù…Ø´Ø¨ÙˆÙ‡ - Ø§Ø­Ø°Ø±"
        verdict_class = "warning"
    else:
        verdict = "âœ… ÙŠØ¨Ø¯Ùˆ Ø¢Ù…Ù†Ø§Ù‹"
        verdict_class = "safe"
    
    return {
        "url": url,
        "domain": syntax["domain"],
        "risk_score": total_risk,
        "verdict": verdict,
        "verdict_class": verdict_class,
        "is_shortened": syntax["is_shortened"],
        "is_suspicious_tld": syntax["is_suspicious_tld"],
        "impersonating": syntax["impersonating"],
        "accessible": content["accessible"],
        "final_url": content["final_url"],
        "redirected": content["redirected"],
        "page_title": content["page_title"],
        "content_type": content["content_type"],
        "fields_detected": content["fields_detected"],
        "arabic_description": content["arabic_description"],
        "content_summary": content["content_summary"],
        "flags": all_flags
    }


async def scan_all_urls_deep(text: str) -> Dict:
    """ÙØ­Øµ ÙƒÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¨Ø§Ù„Ø¹Ù…Ù‚"""
    urls = extract_urls(text)
    
    if not urls:
        return {
            "total_urls": 0,
            "dangerous_urls": 0,
            "urls": [],
            "overall_risk": 0,
            "summary": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø±ÙˆØ§Ø¨Ø·"
        }
    
    results = []
    max_risk = 0
    dangerous_count = 0
    
    for url in urls[:5]:
        analysis = await full_link_analysis(url)
        results.append(analysis)
        if analysis["risk_score"] > max_risk:
            max_risk = analysis["risk_score"]
        if analysis["risk_score"] >= 50:
            dangerous_count += 1
    
    if dangerous_count > 0:
        summary = f"ðŸš¨ ØªÙ… Ø§ÙƒØªØ´Ø§Ù {dangerous_count} Ø±Ø§Ø¨Ø· Ø®Ø·ÙŠØ±!"
    elif max_risk >= 40:
        summary = "âš ï¸ Ø¨Ø¹Ø¶ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ø´Ø¨ÙˆÙ‡Ø©"
    else:
        summary = "âœ… Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ØªØ¨Ø¯Ùˆ Ø¢Ù…Ù†Ø©"
    
    return {
        "total_urls": len(urls),
        "dangerous_urls": dangerous_count,
        "urls": results,
        "overall_risk": max_risk,
        "summary": summary
    }


def scan_all_urls(text: str) -> Dict:
    """ÙØ­Øµ Ø³Ø±ÙŠØ¹ Ø¨Ø¯ÙˆÙ† ÙØªØ­"""
    urls = extract_urls(text)
    if not urls:
        return {"total_urls": 0, "dangerous_urls": 0, "urls": [], "overall_risk": 0}
    
    results = []
    max_risk = 0
    dangerous_count = 0
    
    for url in urls[:10]:
        analysis = analyze_url_syntax(url)
        results.append(analysis)
        if analysis["risk_score"] > max_risk:
            max_risk = analysis["risk_score"]
        if analysis["risk_score"] >= 50:
            dangerous_count += 1
    
    return {
        "total_urls": len(urls),
        "dangerous_urls": dangerous_count,
        "urls": results,
        "overall_risk": max_risk
    }
