"""
Ù†Ù…ÙˆØ°Ø¬ ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ù„ÙƒØ´Ù Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„
Machine Learning Model for Fraud Detection

Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙŠØ´Ø±Ø­ ÙƒÙŠÙ Ù†Ø¨Ù†ÙŠ Ù†Ù…ÙˆØ°Ø¬ ML Ù„Ù„Ù…Ø´Ø±ÙˆØ¹
"""

import os
import pickle
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
DATA_PATH = "data/training_data.csv"
MODEL_PATH = "models/fraud_model.pkl"
VECTORIZER_PATH = "models/vectorizer.pkl"


class FraudDetectionModel:
    """
    Ù†Ù…ÙˆØ°Ø¬ ÙƒØ´Ù Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Random Forest + TF-IDF
    
    Ø§Ù„Ø®Ø·ÙˆØ§Øª:
    1. ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… (TF-IDF)
    2. ØªØ¯Ø±ÙŠØ¨ Random Forest Ø¹Ù„Ù‰ Ø§Ù„ØªØµÙ†ÙŠÙ
    3. Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ø§Ø­Ù‚Ø§Ù‹
    """
    
    def __init__(self):
        # TF-IDF: ÙŠØ­ÙˆÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ vector Ù…Ù† Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
        # - max_features: Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ ÙƒÙ„Ù…Ø§Øª
        # - ngram_range: ÙƒÙ„Ù…Ø§Øª ÙØ±Ø¯ÙŠØ© ÙˆØ«Ù†Ø§Ø¦ÙŠØ©
        self.vectorizer = TfidfVectorizer(
            max_features=3000,
            ngram_range=(1, 2),  # "Ø¨Ø·Ø§Ù‚Ø©" + "Ø¨Ø·Ø§Ù‚Ø© Ù…Ø¬Ù…Ø¯Ø©"
            min_df=1
        )
        
        # Random Forest: Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„ØªØµÙ†ÙŠÙ
        # - n_estimators: Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø´Ø¬Ø§Ø±
        # - class_weight: Ù„Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.model = RandomForestClassifier(
            n_estimators=100,
            max_depth=20,
            class_weight='balanced',
            random_state=42
        )
        
        self.is_trained = False
    
    def train(self, data_path: str = DATA_PATH):
        """
        ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        
        Args:
            data_path: Ù…Ø³Ø§Ø± Ù…Ù„Ù CSV
        
        Returns:
            dict: Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (accuracy, report)
        """
        print("ğŸ“š Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        df = pd.read_csv(data_path)
        print(f"   Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª: {len(df)}")
        print(f"   Ø§Ø­ØªÙŠØ§Ù„: {len(df[df['label']==1])}")
        print(f"   Ø¢Ù…Ù†: {len(df[df['label']==0])}")
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (80% ØªØ¯Ø±ÙŠØ¨ØŒ 20% Ø§Ø®ØªØ¨Ø§Ø±)
        X = df['text']
        y = df['label']
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, 
            test_size=0.2, 
            random_state=42,
            stratify=y  # Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
        )
        
        print("\nğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… (TF-IDF)...")
        X_train_vec = self.vectorizer.fit_transform(X_train)
        X_test_vec = self.vectorizer.transform(X_test)
        
        print(f"   Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {X_train_vec.shape}")
        
        print("\nğŸ§  Ø¬Ø§Ø±ÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
        self.model.fit(X_train_vec, y_train)
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        print("\nğŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:")
        y_pred = self.model.predict(X_test_vec)
        
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, target_names=['Ø¢Ù…Ù†', 'Ø§Ø­ØªÙŠØ§Ù„'])
        
        print(f"\n   Ø§Ù„Ø¯Ù‚Ø©: {accuracy * 100:.1f}%")
        print(f"\n{report}")
        
        self.is_trained = True
        
        return {
            "accuracy": accuracy,
            "report": report,
            "train_size": len(X_train),
            "test_size": len(X_test)
        }
    
    def predict(self, text: str) -> dict:
        """
        ØªØ­Ù„ÙŠÙ„ Ù†Øµ Ø¬Ø¯ÙŠØ¯
        
        Args:
            text: Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡
        
        Returns:
            dict: Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„
        """
        if not self.is_trained:
            return {
                "is_fraud": False,
                "confidence": 0,
                "risk_score": 0,
                "error": "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø±Ø¨"
            }
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ vector
        text_vec = self.vectorizer.transform([text])
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤
        prediction = self.model.predict(text_vec)[0]
        probabilities = self.model.predict_proba(text_vec)[0]
        
        # Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„
        fraud_prob = probabilities[1] if len(probabilities) > 1 else 0
        
        return {
            "is_fraud": bool(prediction),
            "confidence": float(max(probabilities)),
            "fraud_probability": float(fraud_prob),
            "risk_score": int(fraud_prob * 100)
        }
    
    def save(self, model_path: str = MODEL_PATH, vectorizer_path: str = VECTORIZER_PATH):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        
        with open(model_path, 'wb') as f:
            pickle.dump(self.model, f)
        
        with open(vectorizer_path, 'wb') as f:
            pickle.dump(self.vectorizer, f)
        
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: {model_path}")
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù€ Vectorizer ÙÙŠ: {vectorizer_path}")
    
    def load(self, model_path: str = MODEL_PATH, vectorizer_path: str = VECTORIZER_PATH):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            with open(model_path, 'rb') as f:
                self.model = pickle.load(f)
            
            with open(vectorizer_path, 'rb') as f:
                self.vectorizer = pickle.load(f)
            
            self.is_trained = True
            print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­")
            return True
        except FileNotFoundError:
            print("âš ï¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø£ÙˆÙ„Ø§Ù‹")
            return False
    
    def get_important_words(self, top_n: int = 20):
        """Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„ØªØµÙ†ÙŠÙ"""
        if not self.is_trained:
            return []
        
        feature_names = self.vectorizer.get_feature_names_out()
        importances = self.model.feature_importances_
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
        indices = importances.argsort()[::-1][:top_n]
        
        words = []
        for i in indices:
            words.append({
                "word": feature_names[i],
                "importance": float(importances[i])
            })
        
        return words


# ==================== Ù„Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± ====================
if __name__ == "__main__":
    print("=" * 50)
    print("ğŸ›¡ï¸ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø£Ù…Ø§Ù† Ù„ÙƒØ´Ù Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„")
    print("=" * 50)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = FraudDetectionModel()
    
    # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    results = model.train()
    
    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model.save()
    
    # Ø§Ø®ØªØ¨Ø§Ø±
    print("\n" + "=" * 50)
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:")
    print("=" * 50)
    
    test_texts = [
        "ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø¨Ø·Ø§Ù‚ØªÙƒØŒ Ø­Ø¯Ø« Ø¨ÙŠØ§Ù†Ø§ØªÙƒ ÙÙˆØ±Ø§Ù‹ Ø¹Ø¨Ø± Ø§Ù„Ø±Ø§Ø¨Ø·: bank.xyz",
        "Ù…Ø¨Ø±ÙˆÙƒ! Ø±Ø¨Ø­Øª Ù…Ù„ÙŠÙˆÙ† Ø±ÙŠØ§Ù„ØŒ Ø£Ø±Ø³Ù„ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ",
        "ØªØ°ÙƒÙŠØ±: Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„ÙØ±ÙŠÙ‚ ØºØ¯Ø§Ù‹ Ø§Ù„Ø³Ø§Ø¹Ø© 10",
        "Your account suspended. Click here: verify.top"
    ]
    
    for text in test_texts:
        result = model.predict(text)
        status = "ğŸš¨ Ø§Ø­ØªÙŠØ§Ù„" if result["is_fraud"] else "âœ… Ø¢Ù…Ù†"
        print(f"\n{status} ({result['risk_score']}%)")
        print(f"   Ø§Ù„Ù†Øµ: {text[:50]}...")
    
    # Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª
    print("\n" + "=" * 50)
    print("ğŸ“Š Ø£Ù‡Ù… 10 ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„ØªØµÙ†ÙŠÙ:")
    print("=" * 50)
    
    words = model.get_important_words(10)
    for w in words:
        print(f"   {w['word']}: {w['importance']:.4f}")
